{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Model Selection and Validation\n",
      "\n",
      "As with Bayesian inference, model selection and validation are fundamental steps in statistical learning applications. In particular, we wish to select the model that performs optimally, both wish respect to the training data and to external data. \n",
      "\n",
      "Depending on the type of learning method we use, we may be interested in one or more of the following:\n",
      "\n",
      "* how many variables should be included in the model?\n",
      "* what hyperparameter values should be used in fitting the model?\n",
      "* how many groups should be use to cluster our data?\n",
      "\n",
      "\n",
      "The course textbook includes a dataset for salmon spawning success. If we plot the number of recruits against the number of spawners, we see a distinct positive relationship, as we would expect. The question is, *what sort of polynomial relationship best describes the relationship?*"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "\n",
      "salmon = pd.read_table(\"../data/textbook/salmon.dat\", sep='\\s*', index_col=0)\n",
      "plt.scatter(x=salmon.spawners, y=salmon.recruits)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IOError",
       "evalue": "[Errno 2] No such file or directory: '../data/textbook/salmon.dat'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-1-ae6faaf9d491>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msalmon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/textbook/salmon.dat\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\s*'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msalmon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawners\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msalmon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecruits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/jonathandinu/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, dialect, compression, doublequote, escapechar, quotechar, quoting, skipinitialspace, lineterminator, header, index_col, names, prefix, skiprows, skipfooter, skip_footer, na_values, na_fvalues, true_values, false_values, delimiter, converters, dtype, usecols, engine, delim_whitespace, as_recarray, na_filter, compact_ints, use_unsigned, low_memory, buffer_lines, warn_bad_lines, error_bad_lines, keep_default_na, thousands, comment, decimal, parse_dates, keep_date_col, dayfirst, date_parser, memory_map, nrows, iterator, chunksize, verbose, encoding, squeeze, mangle_dupe_cols, tupleize_cols, infer_datetime_format)\u001b[0m\n\u001b[1;32m    418\u001b[0m                     infer_datetime_format=infer_datetime_format)\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/jonathandinu/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/jonathandinu/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    500\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_options_with_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/jonathandinu/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python-fwf'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m                 \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFixedWidthFieldParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/jonathandinu/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, **kwds)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m             f = com._get_handle(f, 'r', encoding=self.encoding,\n\u001b[0;32m-> 1285\u001b[0;31m                                 compression=self.compression)\n\u001b[0m\u001b[1;32m   1286\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrap_compressed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/jonathandinu/anaconda/lib/python2.7/site-packages/pandas/core/common.pyc\u001b[0m in \u001b[0;36m_get_handle\u001b[0;34m(path, mode, encoding, compression)\u001b[0m\n\u001b[1;32m   2312\u001b[0m                 \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'replace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2313\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2314\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2316\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '../data/textbook/salmon.dat'"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/jonathandinu/anaconda/lib/python2.7/site-packages/pytz/__init__.py:35: UserWarning: Module argparse was already imported from /Users/jonathandinu/anaconda/python.app/Contents/lib/python2.7/argparse.pyc, but /Users/jonathandinu/anaconda/lib/python2.7/site-packages is being added to sys.path\n",
        "  from pkg_resources import resource_stream\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One the one extreme, a linear relationship is underfit; at the other, we see that including a very large number of polynomial terms is clearly overfitting the data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "fig, axes = plt.subplots(1, 2, figsize=(14,6))\n",
      "\n",
      "xvals = np.arange(salmon.spawners.min(), salmon.spawners.max())\n",
      "\n",
      "fit1 = np.polyfit(salmon.spawners, salmon.recruits, 1)\n",
      "p1 = np.poly1d(fit1)\n",
      "axes[0].plot(xvals, p1(xvals))\n",
      "axes[0].scatter(x=salmon.spawners, y=salmon.recruits)\n",
      "\n",
      "fit15 = np.polyfit(salmon.spawners, salmon.recruits, 15)\n",
      "p15 = np.poly1d(fit15)\n",
      "axes[1].plot(xvals, p15(xvals))\n",
      "axes[1].scatter(x=salmon.spawners, y=salmon.recruits)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can select an appropriate polynomial order for the model using **cross-validation**, in which we hold out a testing subset from our dataset, fit the model to the remaining data, and evaluate its performance on the held-out subset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "xtrain, xtest, ytrain, ytest = train_test_split(salmon.spawners, \n",
      "                            salmon.recruits, test_size=0.3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A natural criterion to evaluate model performance is root mean square error."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def rmse(x, y, coefs):\n",
      "    yfit = np.polyval(coefs, x)\n",
      "    return np.sqrt(np.mean((y - yfit) ** 2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can now evaluate the model at varying polynomial degrees, and compare their fit."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# suppress warnings from Polyfit\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore', message='Polyfit*')\n",
      "\n",
      "degrees = np.arange(12)\n",
      "train_err = np.zeros(len(degrees))\n",
      "validation_err = np.zeros(len(degrees))\n",
      "\n",
      "for i, d in enumerate(degrees):\n",
      "    p = np.polyfit(xtrain, ytrain, d)\n",
      "\n",
      "    train_err[i] = rmse(xtrain, ytrain, p)\n",
      "    validation_err[i] = rmse(xtest, ytest, p)\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "\n",
      "ax.plot(degrees, validation_err, lw=2, label = 'cross-validation error')\n",
      "ax.plot(degrees, train_err, lw=2, label = 'training error')\n",
      "\n",
      "ax.legend(loc=0)\n",
      "ax.set_xlabel('degree of fit')\n",
      "ax.set_ylabel('rms error')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the cross-validation above, notice that the error is high for both very low and very high polynomial values, while training error declines monotonically with degree. The cross-validation error is composed of two components: **bias** and **variance**. When a model is underfit, bias is low but variance is high, while when a model is overfit, the reverse is true.\n",
      "\n",
      "One can show that the MSE decomposes into a sum of the bias (squared) and variance of the estimator:\n",
      "\n",
      "$$\\begin{aligned}\n",
      "\\text{Var}(\\hat{\\theta}) &= E[\\hat{\\theta} - \\theta]^2 - (E[\\hat{\\theta} - \\theta])^2 \\\\\n",
      "\\Rightarrow E[\\hat{\\theta} - \\theta]^2 &= \\text{Var}(\\hat{\\theta}) + \\text{Bias}(\\hat{\\theta})^2\n",
      "\\end{aligned}$$\n",
      "\n",
      "The training error, on the other hand, does not have this tradeoff; it will always decrease (or at least, never increase) as variables (polynomial terms) are added to the model."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Information-theoretic Model Selection\n",
      "\n",
      "One approach to model selection is to use an information-theoretic criterion to identify the most appropriate model. Akaike (1973) found a formal relationship between Kullback-Leibler information (a dominant paradigm in information and coding theory) and likelihood theory. Akaike's Information Criterion (AIC) is an estimator of expected relative K-L information based on the maximized log-likelihood function, corrected for asymptotic bias. \n",
      "\n",
      "$$\\text{AIC} = \u22122 \\log(L(\\theta|data)) + 2K$$\n",
      "\n",
      "AIC balances the fit of the model (in terms of the likelihood) with the number of parameters required to achieve that fit. We can easily calculate AIC from the residual sums of squares as:\n",
      "\n",
      "$$\\text{AIC} = n \\log(\\text{RSS}/n) + 2k$$\n",
      "\n",
      "where $k$ is the number of parameters in the model. Notice that as the number of parameters increase, the residual sum of squares goes down, but the second term (a penalty) increases.\n",
      "\n",
      "To apply AIC to a model selection problem, we choose the model that has the lowest AIC value.\n",
      "\n",
      "[AIC can be shown to be equivalent to leave-one-out cross-validation](http://www.jstor.org/stable/2984877)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "aic = lambda rss, n, k: n*np.log(float(rss)/n) + 2*k"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can use AIC to select the appropriate polynomial degree."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "aic_values = np.zeros(len(degrees))\n",
      "params = np.zeros((len(degrees), len(degrees)))\n",
      "\n",
      "for i, d in enumerate(degrees):\n",
      "    p, residuals, rank, singular_values, rcond = np.polyfit(\n",
      "                                xtrain, ytrain, d, full=True)\n",
      "    aic_values[i] = aic((residuals).sum(), len(xtrain), d+1)\n",
      "    params[i, :(d+1)] = p\n",
      "\n",
      "plt.plot(degrees, aic_values, lw=2)\n",
      "plt.xlabel('degree of fit')\n",
      "plt.ylabel('AIC')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For ease of interpretation, AIC values can be transformed into model weights via:\n",
      "\n",
      "$$w_i = \\frac{\\exp(-\\frac{1}{2} \\text{AIC}_i)}{\\sum_{m=1}^M \\exp(-\\frac{1}{2} \\text{AIC}_m)}$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "aic_trans = np.exp(-0.5*aic_values)\n",
      "aic_probs = aic_trans/aic_trans.sum()\n",
      "aic_probs.round(2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For some problems, we can use AIC weights to perform multimodel inference, whereby we use model weights to calculate model-averaged parameter estimates, thereby accounting for model selection uncertainty.\n",
      "\n",
      "As an example, consider the body fat dataset that is used in Chapter 12 of the textbook. It measures the percentage of body fat for 251 men, estimated using an underwater weighing technique.  In addition, the variables age, weight, height, and ten body circumference measurements were recorded for each subject."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bodyfat = pd.read_table(\"../data/textbook/bodyfat.dat\", sep='\\s+')\n",
      "bodyfat.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To illustrate model selection, we will consider 5 competing models consisting of different subsets of available covariates."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "subsets = [['weight', 'height', 'neck', 'chest', 'abd', 'hip', 'thigh', \n",
      "                            'knee', 'ankle', 'biceps'],\n",
      "           ['weight', 'height', 'neck', 'chest', 'abd', 'hip', 'thigh', \n",
      "                            'knee'],\n",
      "           ['weight', 'height', 'neck', 'chest', 'abd', 'hip'],\n",
      "           ['weight', 'height', 'neck'],\n",
      "           ['weight']]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fit = pd.ols(y=bodyfat['fat'], x=bodyfat[subsets[3]])\n",
      "fit"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "k0 = len(subsets[0])\n",
      "aic_values = np.zeros(len(subsets))\n",
      "params = np.zeros((len(subsets), k0))\n",
      "\n",
      "for i, s in enumerate(subsets):\n",
      "    x = bodyfat[s]\n",
      "    y = bodyfat['fat']\n",
      "    fit = pd.ols(y=y, x=x)\n",
      "    aic_values[i] = fit.sm_ols.aic\n",
      "    params[i, :len(s)] = fit.beta[:-1]\n",
      "\n",
      "plt.plot([len(s) for s in subsets], aic_values, lw=2)\n",
      "plt.xlabel('covariates')\n",
      "plt.ylabel('AIC')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "p_best = params[np.where(aic_values==aic_values.min())]\n",
      "p_best.round(2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "aic_trans = np.exp(-0.5*aic_values)\n",
      "aic_probs = aic_trans/aic_trans.sum()\n",
      "aic_probs.round(2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "p_weighted = ((params.T * aic_probs).T).sum(0)\n",
      "p_weighted.round(2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## K-fold Cross-validation\n",
      "\n",
      "In **k-fold cross-validation**, the training set is split into *k* smaller sets. Then, for each of the k \"folds\":\n",
      "\n",
      "1. trained model on *k-1* of the folds as training data\n",
      "2. validate this model the remaining fold, using an appropriate metric\n",
      "\n",
      "The performance measure reported by k-fold CV is then the average of the *k* computed values. This approach can be computationally expensive, but does not waste too much data, which is an advantage over having a fixed test subset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import cross_val_score, KFold\n",
      "\n",
      "nfolds = 5\n",
      "fig, axes = plt.subplots(1, nfolds, figsize=(14,4))\n",
      "for i, fold in enumerate(KFold(len(salmon), n_folds=nfolds, \n",
      "                               shuffle=True)):\n",
      "    training, validation = fold\n",
      "    y, x = salmon.values[training].T\n",
      "    axes[i].plot(x, y, 'ro')\n",
      "    y, x = salmon.values[validation].T\n",
      "    axes[i].plot(x, y, 'bo')\n",
      "plt.tight_layout()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import warnings\n",
      "warnings.filterwarnings('ignore', message='Polyfit*')\n",
      "\n",
      "k = 5\n",
      "degrees = np.arange(8)\n",
      "k_fold_err = np.empty(len(degrees))\n",
      "\n",
      "for i, d in enumerate(degrees):\n",
      "    \n",
      "    error = np.empty(k)\n",
      "    \n",
      "    #for j, fold in enumerate(gen_k_folds(salmon, k)):\n",
      "    for j, fold in enumerate(KFold(len(salmon), n_folds=k)):\n",
      "\n",
      "        training, validation = fold\n",
      "        \n",
      "        y_train, x_train = salmon.values[training].T\n",
      "        y_test, x_test = salmon.values[validation].T\n",
      "        \n",
      "        p = np.polyfit(x_train, y_train, d)\n",
      "        \n",
      "        error[j] = rmse(x_test, y_test, p)\n",
      "\n",
      "    k_fold_err[i] = error.mean()\n",
      "        \n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "\n",
      "ax.plot(degrees, k_fold_err, lw=2)\n",
      "ax.set_xlabel('degree of fit')\n",
      "ax.set_ylabel('average rms error')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'np' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-2-80999030a72b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdegrees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mk_fold_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdegrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If the model shows high **bias**, the following actions might help:\n",
      "\n",
      "- **Add more features**. In our example of predicting home prices,\n",
      "  it may be helpful to make use of information such as the neighborhood\n",
      "  the house is in, the year the house was built, the size of the lot, etc.\n",
      "  Adding these features to the training and test sets can improve\n",
      "  a high-bias estimator\n",
      "- **Use a more sophisticated model**. Adding complexity to the model can\n",
      "  help improve on bias. For a polynomial fit, this can be accomplished\n",
      "  by increasing the degree d. Each learning technique has its own\n",
      "  methods of adding complexity.\n",
      "- **Decrease regularization**. Regularization is a technique used to impose\n",
      "  simplicity in some machine learning models, by adding a penalty term that\n",
      "  depends on the characteristics of the parameters. If a model has high bias,\n",
      "  decreasing the effect of regularization can lead to better results.\n",
      "  \n",
      "If the model shows **high variance**, the following actions might help:\n",
      "\n",
      "- **Use fewer features**. Using a feature selection technique may be\n",
      "  useful, and decrease the over-fitting of the estimator.\n",
      "- **Use a simpler model**.  Model complexity and over-fitting go hand-in-hand.\n",
      "- **Use more training samples**. Adding training samples can reduce\n",
      "  the effect of over-fitting, and lead to improvements in a high\n",
      "  variance estimator.\n",
      "- **Increase regularization**. Regularization is designed to prevent\n",
      "  over-fitting. In a high-variance model, increasing regularization\n",
      "  can lead to better results."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Regularization\n",
      "\n",
      "The `scikit-learn` package includes a built-in dataset of diabetes progression, taken from [Efron *et al.* (2003)](http://arxiv.org/pdf/math/0406456.pdf), which includes a set of 10 normalized predictors."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import datasets\n",
      "\n",
      "# Predictors: \"age\" \"sex\" \"bmi\" \"map\" \"tc\"  \"ldl\" \"hdl\" \"tch\" \"ltg\" \"glu\"\n",
      "diabetes = datasets.load_diabetes()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's examine how a linear regression model performs across a range of sample sizes."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "diabetes['data'].shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "(442, 10)"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import cross_validation, linear_model\n",
      "\n",
      "def plot_learning_curve(estimator, label=None):\n",
      "    scores = list()\n",
      "    train_sizes = np.linspace(10, 200, 10).astype(np.int)\n",
      "    for train_size in train_sizes:\n",
      "        test_error = cross_validation.cross_val_score(estimator, diabetes['data'], diabetes['target'],\n",
      "                        cv=cross_validation.ShuffleSplit(train_size=train_size, \n",
      "                                                         test_size=200, \n",
      "                                                         n=len(diabetes['target']),\n",
      "                                                         random_state=0)\n",
      "                        )\n",
      "        scores.append(test_error)\n",
      "\n",
      "    plt.plot(train_sizes, np.mean(scores, axis=1), label=label or estimator.__class__.__name__)\n",
      "    plt.ylim(0, 1)\n",
      "    plt.ylabel('Explained variance on test set')\n",
      "    plt.xlabel('Training set size')\n",
      "    plt.legend(loc='best')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_learning_curve(linear_model.LinearRegression())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "global name 'np' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-6-3e01f355bba1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_learning_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m<ipython-input-5-831013f60527>\u001b[0m in \u001b[0;36mplot_learning_curve\u001b[0;34m(estimator, label)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_learning_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtrain_size\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_sizes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         test_error = cross_validation.cross_val_score(estimator, diabetes['data'], diabetes['target'],\n",
        "\u001b[0;31mNameError\u001b[0m: global name 'np' is not defined"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice the linear regression is not defined for scenarios where the number of features/parameters exceeds the number of observations. It performs poorly as long as the number of sample is not several times the number of features.\n",
      "\n",
      "One approach for dealing with overfitting is to **regularize** the regession model.\n",
      "\n",
      "The **ridge estimator** is a simple, computationally efficient regularization for linear regression.\n",
      "\n",
      "$$\\hat{\\beta}^{ridge} = \\text{argmin}_{\\beta}\\left\\{\\sum_{i=1}^N (y_i - \\beta_0 - \\sum_{j=1}^k x_{ij} \\beta_j)^2 + \\lambda \\sum_{j=1}^k \\beta_j^2 \\right\\}$$\n",
      "\n",
      "Typically, we are not interested in shrinking the mean, and coefficients are standardized to have zero mean and unit L2 norm. Hence,\n",
      "\n",
      "$$\\hat{\\beta}^{ridge} = \\text{argmin}_{\\beta} \\sum_{i=1}^N (y_i - \\sum_{j=1}^k x_{ij} \\beta_j)^2$$\n",
      "\n",
      "$$\\text{subject to } \\sum_{j=1}^k \\beta_j^2 < \\lambda$$\n",
      "\n",
      "Note that this is *equivalent* to a Bayesian model $y \\sim N(X\\beta, I)$ with a Gaussian prior on the $\\beta_j$:\n",
      "\n",
      "$$\\beta_j \\sim \\text{N}(0, \\lambda)$$\n",
      "\n",
      "The estimator for the ridge regression model is:\n",
      "\n",
      "$$\\hat{\\beta}^{ridge} = (X'X + \\lambda I)^{-1}X'y$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_d"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "diabetes['data']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import preprocessing\n",
      "\n",
      "k = diabetes['data'].shape[1]\n",
      "alphas = np.linspace(0, 4)\n",
      "params = np.zeros((len(alphas), k))\n",
      "for i,a in enumerate(alphas):\n",
      "    X = preprocessing.scale(diabetes['data'])\n",
      "    y = diabetes['target']\n",
      "    fit = linear_model.Ridge(alpha=a, normalize=True).fit(X, y)\n",
      "    params[i] = fit.coef_\n",
      "\n",
      "figure(figsize=(14,6))\n",
      "for param in params.T:\n",
      "    plt.plot(alphas, param)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_learning_curve(linear_model.LinearRegression())\n",
      "plot_learning_curve(linear_model.Ridge())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice that at very small sample sizes, the ridge estimator outperforms the unregularized model.\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for a in [0.001, 0.01, 0.1, 1, 10]:\n",
      "    plot_learning_curve(linear_model.Ridge(a), a)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_learning_curve(linear_model.LinearRegression())\n",
      "plot_learning_curve(linear_model.Ridge())\n",
      "plot_learning_curve(linear_model.RidgeCV())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**The Lasso estimator** is useful to impose sparsity on the coefficients. In other words, it is to be prefered if we believe that many of the features are not relevant.\n",
      "\n",
      "$$\\hat{\\beta}^{lasso} = \\text{argmin}_{\\beta}\\left\\{\\frac{1}{2}\\sum_{i=1}^N (y_i - \\beta_0 - \\sum_{j=1}^k x_{ij} \\beta_j)^2 + \\lambda \\sum_{j=1}^k |\\beta_j| \\right\\}$$\n",
      "\n",
      "or, similarly:\n",
      "\n",
      "$$\\hat{\\beta}^{lasso} = \\text{argmin}_{\\beta} \\frac{1}{2}\\sum_{i=1}^N (y_i - \\sum_{j=1}^k x_{ij} \\beta_j)^2$$\n",
      "$$\\text{subject to } \\sum_{j=1}^k |\\beta_j| < \\lambda$$\n",
      "\n",
      "Note that this is *equivalent* to a Bayesian model $y \\sim N(X\\beta, I)$ with a **Laplace** prior on the $\\beta_j$:\n",
      "\n",
      "$$\\beta_j \\sim \\text{Laplace}(\\lambda) = \\frac{\\lambda}{2}\\exp(-\\lambda|\\beta_j|)$$\n",
      "\n",
      "Note how the Lasso imposes sparseness on the parameter coefficients:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "k = diabetes['data'].shape[1]\n",
      "alphas = np.linspace(0.1, 3)\n",
      "params = np.zeros((len(alphas), k))\n",
      "for i,a in enumerate(alphas):\n",
      "    X = preprocessing.scale(diabetes['data'])\n",
      "    y = diabetes['target']\n",
      "    fit = linear_model.Lasso(alpha=a, normalize=True).fit(X, y)\n",
      "    params[i] = fit.coef_\n",
      "\n",
      "figure(figsize=(14,6))\n",
      "for param in params.T:\n",
      "    plt.plot(alphas, param)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_learning_curve(linear_model.RidgeCV())\n",
      "plot_learning_curve(linear_model.Lasso(0.05))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this example, the ridge estimator performs better than the lasso, but when there are fewer observations, the lasso matches its performance. Otherwise, the variance-reducing effect of the lasso regularization is unhelpful relative to the increase in bias.\n",
      "\n",
      "With the lasso too, me must tune the regularization parameter for good performance. There is a corresponding `LassoCV` function in `scikit-learn`, but it is computationally expensive. To speed it up, we can reduce the number of values explored for the alpha parameter."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_learning_curve(linear_model.RidgeCV())\n",
      "plot_learning_curve(linear_model.LassoCV(n_alphas=10))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Can't decide? **ElasticNet** is a compromise between lasso and ridge regression.\n",
      "\n",
      "$$\\hat{\\beta}^{elastic} = \\text{argmin}_{\\beta}\\left\\{\\frac{1}{2}\\sum_{i=1}^N (y_i - \\beta_0 - \\sum_{j=1}^k x_{ij} \\beta_j)^2 + (1 - \\alpha) \\sum_{j=1}^k \\beta^2_j + \\alpha \\sum_{j=1}^k |\\beta_j| \\right\\}$$\n",
      "\n",
      "where $\\alpha = \\lambda_1/(\\lambda_1 + \\lambda_2)$. Its tuning parameter $\\alpha$ (`l1_ratio` in `scikit-learn`) controls this mixture: when set to 0, ElasticNet is a ridge regression, when set to 1, it is a lasso. The sparser the coefficients, the higher we should set $\\alpha$. \n",
      "\n",
      "Note that $\\alpha$ can also be set by cross-validation, though it is computationally costly."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_learning_curve(linear_model.RidgeCV())\n",
      "plot_learning_curve(linear_model.ElasticNetCV(l1_ratio=.6, n_alphas=10))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Using Cross-validation for Parameter Tuning"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lasso = linear_model.Lasso()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "alphas = np.logspace(-4, -1, 20)\n",
      "\n",
      "scores = np.empty(len(alphas))\n",
      "scores_std = np.empty(len(alphas))\n",
      "\n",
      "for i,alpha in enumerate(alphas):\n",
      "    lasso.alpha = alpha\n",
      "    s = cross_validation.cross_val_score(lasso, diabetes.data, diabetes.target, n_jobs=-1)\n",
      "    scores[i] = s.mean()\n",
      "    scores_std[i] = s.std()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.semilogx(alphas, scores)\n",
      "plt.semilogx(alphas, np.array(scores) + np.array(scores_std)/20, 'b--')\n",
      "plt.semilogx(alphas, np.array(scores) - np.array(scores_std)/20, 'b--')\n",
      "plt.yticks(())\n",
      "plt.ylabel('CV score')\n",
      "plt.xlabel('alpha')\n",
      "plt.axhline(np.max(scores), linestyle='--', color='.5')\n",
      "plt.text(5e-2, np.max(scores)+1e-4, str(np.max(scores).round(3)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "k_fold = cross_validation.KFold(len(diabetes.data), 5)\n",
      "print [lasso.fit(diabetes.data[train], \n",
      "         diabetes.target[train]).alpha for train, _ in k_fold]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Exercise: Very low birthweight infants\n",
      "\n",
      "Compare logistic regression models (using the `linear_model.LogisticRegression` interface) for the VLBW infant database. Use a relevant metric such as the Brier's score as a metric.\n",
      "\n",
      "$$B = \\frac{1}{n} \\sum_{i=1}^n (\\hat{p}_i - y_i)^2$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vlbw = pd.read_csv(\"../data/vlbw.csv\", index_col=0)\n",
      "\n",
      "vlbw = vlbw.replace({'inout':{'born at Duke':0, 'transported':1},\n",
      "             'delivery':{'abdominal':0, 'vaginal':1},\n",
      "             'ivh':{'absent':0, 'present':1, 'possible':1, 'definite':1},\n",
      "             'sex':{'female':0, 'male':1}})\n",
      "\n",
      "vlbw = vlbw[[u'birth', u'exit', u'hospstay', u'lowph', u'pltct', \n",
      "      u'bwt', u'gest', u'meth', \n",
      "      u'toc', u'delivery', u'apg1', u'vent', u'pneumo', u'pda', u'cld', \n",
      "      u'ivh']].dropna()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.preprocessing import normalize\n",
      "\n",
      "y = vlbw.pop('ivh').values\n",
      "X = vlbw"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.grid_search import GridSearchCV\n",
      "cvalues = [0.1, 1, 10,100]\n",
      "grid = GridSearchCV(LogisticRegression(), \n",
      "    param_grid={'C': cvalues}, scoring='average_precision')\n",
      "gf = grid.fit(X, y)\n",
      "gf.grid_scores_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "grid = GridSearchCV(LogisticRegression(), \n",
      "            param_grid={'C': cvalues}, scoring='roc_auc')\n",
      "gf = grid.fit(X, y)\n",
      "gf.grid_scores_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## References\n",
      "\n",
      "Burnham, K. P., & Anderson, D. R. (2002). [Model Selection and Multi-Model Inference: A Practical, Information-theoretic Approach](http://www.amazon.com/Model-Selection-Multimodel-Inference-Information-Theoretic/dp/0387953647). Springer Verlag.\n",
      "\n",
      "Hastie, T., Tibshirani, R., & Friedman, J. H. (2009). [The elements of statistical learning](http://www-stat.stanford.edu/~tibs/ElemStatLearn/). Springer Verlag.\n",
      "\n",
      "Vanderplas, J. [Scikit-learn tutorials for the Scipy 2013 conference](https://github.com/jakevdp/sklearn_scipy2013)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.core.display import HTML\n",
      "def css_styling():\n",
      "    styles = open(\"styles/custom.css\", \"r\").read()\n",
      "    return HTML(styles)\n",
      "css_styling()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}