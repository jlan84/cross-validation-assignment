{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Regression Regularization\n",
      "\n",
      "For this exercise you will be comparing Ridge Regression and LASSO regression to Ordinary Least Squares.  You will also get experience with techniques of cross validation.  We will be using [scikit-learn](http://scikit-learn.org/stable/supervised_learning.html#supervised-learning) to fit our models, do not worry about the details of how the library works though.  We will get into the details of this next week.  Look to the [lecture](lecture.ipynb] notebook for an example of what functions to use with what parameters. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "\n",
      "from sklearn.cross_validation import KFold\n",
      "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
      "from sklearn.linear_model import LassoCV, RidgeCV\n",
      "from sklearn.cross_validation import train_test_split\n",
      "import numpy as np\n",
      "import pylab as pl\n",
      "\n",
      "from sklearn.datasets import load_boston\n",
      "\n",
      "boston = load_boston()\n",
      "X = np.array([np.concatenate((v,[1])) for v in boston.data])\n",
      "Y = boston.target"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print Y[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 24.   21.6  34.7  33.4  36.2  28.7  22.9  27.1  16.5  18.9]\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print X[:2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[  6.32000000e-03   1.80000000e+01   2.31000000e+00   0.00000000e+00\n",
        "    5.38000000e-01   6.57500000e+00   6.52000000e+01   4.09000000e+00\n",
        "    1.00000000e+00   2.96000000e+02   1.53000000e+01   3.96900000e+02\n",
        "    4.98000000e+00   1.00000000e+00]\n",
        " [  2.73100000e-02   0.00000000e+00   7.07000000e+00   0.00000000e+00\n",
        "    4.69000000e-01   6.42100000e+00   7.89000000e+01   4.96710000e+00\n",
        "    2.00000000e+00   2.42000000e+02   1.78000000e+01   3.96900000e+02\n",
        "    9.14000000e+00   1.00000000e+00]]\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Dataset\n",
      "\n",
      "We will be using a [dataset](http://archive.ics.uci.edu/ml/datasets/Housing) from the UCI machine learning Repository for this exercise.  Feel free to play around with any of the others that are [suited](http://archive.ics.uci.edu/ml/datasets.html?format=&task=reg&att=&area=&numAtt=&numIns=&type=&sort=nameUp&view=table) for regression as well.  This dataset is actually containe in scikit-learn's built in datasets."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# EXERCISE: Create a new linear regression model and fit it using the dataset\n",
      "\n",
      "# Create linear regression object\n",
      "linear = LinearRegression()\n",
      "\n",
      "# TODO: Fit the data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def rmse(x, y, coefs):\n",
      "    yfit = np.polyval(coefs, x)\n",
      "    return np.sqrt(np.mean((y - yfit) ** 2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# EXERCISE: Compute the RMSE on the training data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# EXERCISE: Examine the coefficients return from your model.  Maybe make a plot of these.\n",
      "# http://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html#example-linear-model-plot-ols-py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# EXERCISE: Split your data into a training and test set (hold-out set).\n",
      "\n",
      "# Play around with the ratio of these (i.e. 70%/30% train/test, 80%/20% train/test, etc.)\n",
      "# and compute the fit on only the training data. Test the RMSE of your results on the test data."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## K-fold Cross-validation\n",
      "\n",
      "In **k-fold cross-validation**, the training set is split into *k* smaller sets. Then, for each of the k \"folds\":\n",
      "\n",
      "1. trained model on *k-1* of the folds as training data\n",
      "2. validate this model the remaining fold, using an appropriate metric\n",
      "\n",
      "The performance measure reported by k-fold CV is then the average of the *k* computed values. This approach can be computationally expensive, but does not waste too much data, which is an advantage over having a fixed test subset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# EXERCISE: Repeat the above but this time use K-fold cross validation.\n",
      "\n",
      "kf = KFold(len(x), n_folds=10)\n",
      "xval_err = 0\n",
      "for train,test in kf:\n",
      "    linreg.fit(x[train],y[train])\n",
      "    p = np.array([linear_clf.predict(xi) for xi in x[test]])\n",
      "    e = p-y[test]\n",
      "    xval_err += np.dot(e,e)\n",
      "rmse_10cv = np.sqrt(xval_err/len(x))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# EXERCISE: Compare the RMSE for your hold-out set and K-fold cross validation"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# EXERCISE: Plot the learning curve for a standard ordinary least squares regression"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import cross_validation, linear_model\n",
      "\n",
      "# Convenience funciton defined to plot the learning curve\n",
      "def plot_learning_curve(estimator, label=None):\n",
      "    scores = list()\n",
      "    train_sizes = np.linspace(10, 200, 10).astype(np.int)\n",
      "    for train_size in train_sizes:\n",
      "        test_error = cross_validation.cross_val_score(estimator, X, Y,\n",
      "                        cv=cross_validation.ShuffleSplit(train_size=train_size, \n",
      "                                                         test_size=200, \n",
      "                                                         n=len(Y),\n",
      "                                                         random_state=0)\n",
      "                        )\n",
      "        scores.append(test_error)\n",
      "\n",
      "    plt.plot(train_sizes, np.mean(scores, axis=1), label=label or estimator.__class__.__name__)\n",
      "    plt.ylim(0, 1)\n",
      "    plt.ylabel('Explained variance on test set')\n",
      "    plt.xlabel('Training set size')\n",
      "    plt.legend(loc='best')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# EXERCISE: Now that you have experimented with linear regression we will begin exploring Ridge Regression\n",
      "\n",
      "# Fit the same dataset but with a Ridge Regression with an alpha == 0.5 to start"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice the linear regression is not defined for scenarios where the number of features/parameters exceeds the number of observations. It performs poorly as long as the number of sample is not several times the number of features.\n",
      "\n",
      "One approach for dealing with overfitting is to **regularize** the regession model.\n",
      "\n",
      "The **ridge estimator** is a simple, computationally efficient regularization for linear regression.\n",
      "\n",
      "$$\\hat{\\beta}^{ridge} = \\text{argmin}_{\\beta}\\left\\{\\sum_{i=1}^N (y_i - \\beta_0 - \\sum_{j=1}^k x_{ij} \\beta_j)^2 + \\alpha \\sum_{j=1}^k \\beta_j^2 \\right\\}$$\n",
      "\n",
      "Typically, we are not interested in shrinking the mean, and coefficients are standardized to have zero mean and unit L2 norm. Hence,\n",
      "\n",
      "$$\\hat{\\beta}^{ridge} = \\text{argmin}_{\\beta} \\sum_{i=1}^N (y_i - \\sum_{j=1}^k x_{ij} \\beta_j)^2$$\n",
      "\n",
      "$$\\text{subject to } \\sum_{j=1}^k \\beta_j^2 < \\alpha$$\n",
      "\n",
      "Note that this is *equivalent* to a Bayesian model $y \\sim N(X\\beta, I)$ with a Gaussian prior on the $\\beta_j$:\n",
      "\n",
      "$$\\beta_j \\sim \\text{N}(0, \\alpha)$$\n",
      "\n",
      "The estimator for the ridge regression model is:\n",
      "\n",
      "$$\\hat{\\beta}^{ridge} = (X'X + \\alpha I)^{-1}X'y$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# EXERCISE: Make a plot of the training error and the testing error as a function of the alpha paramter"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The regularization of the ridge is a *shrinkage*: the coefficients learned are shrunk towards zero.\n",
      "\n",
      "The amount of regularization is set via the `alpha` parameter of the ridge, which is tunable. The `RidgeCV` method in `scikits-learn` automatically tunes this parameter via cross-validation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# EXERCISE: Plot the parameters (coefficients) of the Ridge regression (y-axis) versus the value of the alpha parameter.  \n",
      "# There will be as many lines as there are parameters.\n",
      "\n",
      "from sklearn import preprocessing\n",
      "\n",
      "k = X.shape[1]\n",
      "alphas = np.linspace(0, 4)\n",
      "params = np.zeros((len(alphas), k))\n",
      "for i,a in enumerate(alphas):\n",
      "    X_data = preprocessing.scale(X)\n",
      "    y = Y\n",
      "    fit = Ridge(alpha=a, normalize=True).fit(X_data, y)\n",
      "    params[i] = fit.coef_\n",
      "\n",
      "figure(figsize=(14,6))\n",
      "for param in params.T:\n",
      "    plt.plot(alphas, param)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# EXERCISE: Plot the learning curve of the Ridge regression with different alpha parameters"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# EXERCISE: Plot the learning curves of the Ridge Regression and Ordinary Least Squares Regression.  Compare these two."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**The Lasso estimator** is useful to impose sparsity on the coefficients. In other words, it is to be prefered if we believe that many of the features are not relevant.\n",
      "\n",
      "$$\\hat{\\beta}^{lasso} = \\text{argmin}_{\\beta}\\left\\{\\frac{1}{2}\\sum_{i=1}^N (y_i - \\beta_0 - \\sum_{j=1}^k x_{ij} \\beta_j)^2 + \\lambda \\sum_{j=1}^k |\\beta_j| \\right\\}$$\n",
      "\n",
      "or, similarly:\n",
      "\n",
      "$$\\hat{\\beta}^{lasso} = \\text{argmin}_{\\beta} \\frac{1}{2}\\sum_{i=1}^N (y_i - \\sum_{j=1}^k x_{ij} \\beta_j)^2$$\n",
      "$$\\text{subject to } \\sum_{j=1}^k |\\beta_j| < \\lambda$$\n",
      "\n",
      "Note that this is *equivalent* to a Bayesian model $y \\sim N(X\\beta, I)$ with a **Laplace** prior on the $\\beta_j$:\n",
      "\n",
      "$$\\beta_j \\sim \\text{Laplace}(\\lambda) = \\frac{\\lambda}{2}\\exp(-\\lambda|\\beta_j|)$$\n",
      "\n",
      "Note how the Lasso imposes sparseness on the parameter coefficients:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Repeat the above steps with LASSO Regression"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Make a plot of the training error and the testing error as a function of the alpha paramter"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# EXERCISE: Plot the parameters (coefficients) of the LASSO regression (y-axis) versus the value of the alpha parameter.  \n",
      "# There will be as many lines as there are parameters."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# EXERCISE: Plot the learning curve of the LASSO regression with different alpha parameters\n",
      "\n",
      "k = X.shape[1]\n",
      "alphas = np.linspace(0.1, 3)\n",
      "params = np.zeros((len(alphas), k))\n",
      "for i,a in enumerate(alphas):\n",
      "    X_data = preprocessing.scale(X)\n",
      "    y = Y\n",
      "    fit = linear_model.Lasso(alpha=a, normalize=True).fit(X_data, y)\n",
      "    params[i] = fit.coef_\n",
      "\n",
      "figure(figsize=(14,6))\n",
      "for param in params.T:\n",
      "    plt.plot(alphas, param)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# EXERCISE: Plot the learning curves of the Ridge Regression, LASSO Regression, and \n",
      "# Ordinary Least Squares Regression.  Compare these all."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}